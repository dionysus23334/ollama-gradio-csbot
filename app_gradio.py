# -*- coding: utf-8 -*-
# file: app_gradio.py
import os
import json
import time
import requests
import gradio as gr
from typing import List, Dict, Tuple, Any
from dotenv import load_dotenv

# è¯»å– .env
load_dotenv()

OLLAMA = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434").rstrip("/")
MODEL  = os.getenv("OLLAMA_MODEL", "llama3.1")
DEFAULT_TEMPERATURE = float(os.getenv("TEMPERATURE", "0.7"))

PROMPT_FILE = "prompt.txt"
DIALOGUES_FILE = "dialogues.json"

def read_file(path: str, default: str = "") -> str:
    if not os.path.exists(path):
        return default
    with open(path, "r", encoding="utf-8") as f:
        return f.read()

def write_json(path: str, data: Any):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def append_dialogue(path: str, user_text: str, assistant_text: str):
    data = {"examples": []}
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            try:
                data = json.load(f)
            except Exception:
                data = {"examples": []}
    data.setdefault("examples", []).append({"user": user_text, "assistant": assistant_text})
    write_json(path, data)

def stream_chat(messages: List[Dict[str, str]], temperature: float = DEFAULT_TEMPERATURE):
    """
    è°ƒç”¨ Ollama /api/chatï¼Œé‡‡ç”¨ NDJSON æµå¼è¿”å›ã€‚
    messages: [{"role":"system"|"user"|"assistant","content":"..."}]
    """
    url = f"{OLLAMA}/api/chat"
    payload = {
        "model": MODEL,
        "messages": messages,
        "stream": True,
        "options": {"temperature": temperature}
    }
    with requests.post(url, json=payload, stream=True, timeout=600) as resp:
        resp.raise_for_status()
        partial = ""
        for line in resp.iter_lines(decode_unicode=True):
            if not line:
                continue
            try:
                data = json.loads(line)
            except json.JSONDecodeError:
                continue
            msg = data.get("message", {})
            token = msg.get("content")
            if token:
                partial += token
                yield partial
            if data.get("done"):
                break

def build_messages(system_prompt: str, history: List[Tuple[str, str]], user_input: str):
    messages: List[Dict[str, str]] = []
    if system_prompt.strip():
        messages.append({"role": "system", "content": system_prompt})
    for role, content in history:
        messages.append({"role": role, "content": content})
    messages.append({"role": "user", "content": user_input})
    return messages

def on_send(user_input: str, chat_state: List[Tuple[str, str]],
            sys_prompt_text: str, temperature: float,
            model_name: str, base_url: str, chat_ui):
    """
    Gradio äº‹ä»¶å›è°ƒï¼šæµå¼ç”Ÿæˆå¹¶å®æ—¶æ›´æ–° Chatbotã€‚
    chat_state: [("user","..."),("assistant","...")]
    """
    global MODEL, OLLAMA
    MODEL = model_name.strip() or MODEL
    OLLAMA = (base_url.strip() or OLLAMA).rstrip("/")

    messages = build_messages(sys_prompt_text, chat_state, user_input)
    final = ""
    for partial in stream_chat(messages, temperature=temperature):
        final = partial
        tmp = chat_ui + [(user_input, final)]
        yield tmp, chat_state

    chat_state = chat_state + [("user", user_input), ("assistant", final)]
    # å°†æœ¬è½®å¯¹è¯è¿½åŠ ä¿å­˜
    try:
        append_dialogue(DIALOGUES_FILE, user_input, final)
    except Exception:
        pass
    yield chat_ui + [(user_input, final)], chat_state

def on_clear():
    return [], []

def on_load_files():
    sys_prompt_text = read_file(PROMPT_FILE, default="ä½ æ˜¯ä¸€åç®€æ´ã€ç¨³é‡ã€è®²é€»è¾‘çš„ä¸­æ–‡åŠ©æ‰‹ã€‚å›ç­”åº”åˆ†ç‚¹ã€ç›´è¾¾è¦ç‚¹ã€‚")
    examples = []
    if os.path.exists(DIALOGUES_FILE):
        try:
            data = json.loads(read_file(DIALOGUES_FILE, default='{"examples": []}'))
            examples = data.get("examples", [])
        except Exception:
            examples = []
    return sys_prompt_text, json.dumps(examples, ensure_ascii=False, indent=2)

with gr.Blocks(title="Ollama Llama3.1 å¯¹è¯è¯•éªŒå°") as demo:
    gr.Markdown("## ğŸ§ª Ollama `llama3.1` æœ¬åœ°å¯¹è¯è¯•éªŒå°\n- æ”¯æŒï¼š**æµå¼ï¼ˆStreamingï¼‰**ã€å¤šè½®è®°å¿†ã€åœ¨çº¿ç¼–è¾‘ **ç³»ç»Ÿæç¤ºè¯ï¼ˆSystem Promptï¼‰**ã€‚\n- å³ä¸Šè§’å¯åˆ‡æ¢æ¨¡å‹åä¸æœåŠ¡åœ°å€ã€‚")

    with gr.Row():
        sys_prompt_text = gr.Textbox(label="System Promptï¼ˆç³»ç»Ÿæç¤ºï¼‰", lines=12)
        with gr.Column():
            model_name = gr.Textbox(value=MODEL, label="æ¨¡å‹åï¼ˆå¦‚ llama3.1ï¼‰")
            base_url   = gr.Textbox(value=OLLAMA, label="Ollama æœåŠ¡åœ°å€")
            temperature = gr.Slider(0.0, 1.5, value=DEFAULT_TEMPERATURE, step=0.1, label="Temperatureï¼ˆé‡‡æ ·æ¸©åº¦ï¼‰")
            clear_btn  = gr.Button("æ¸…ç©ºå¯¹è¯")
            load_btn   = gr.Button("ä»æ–‡ä»¶è½½å…¥ prompt.txt / dialogues.json")

    chat_ui = gr.Chatbot(label="å¯¹è¯çª—å£", height=420)
    user_in = gr.Textbox(label="è¾“å…¥ä½ çš„é—®é¢˜", placeholder="åœ¨è¿™é‡Œè¾“å…¥é—®é¢˜ï¼Œå›è½¦æˆ–ç‚¹å‡»å‘é€", lines=2)
    send_btn = gr.Button("å‘é€ / Send")

    state = gr.State([])  # [("user","..."), ("assistant","...")]

    # äº‹ä»¶ç»‘å®šï¼ˆå‘é€ï¼‰
    send_btn.click(
        on_send,
        inputs=[user_in, state, sys_prompt_text, temperature, model_name, base_url, chat_ui],
        outputs=[chat_ui, state]
    )
    user_in.submit(
        on_send,
        inputs=[user_in, state, sys_prompt_text, temperature, model_name, base_url, chat_ui],
        outputs=[chat_ui, state]
    )

    # æ¸…ç©º
    clear_btn.click(on_clear, outputs=[chat_ui, state])

    # ä»æ–‡ä»¶è½½å…¥
    load_btn.click(on_load_files, outputs=[sys_prompt_text, gr.Code(label="dialogues.json é¢„è§ˆ")])

if __name__ == "__main__":
    # åŸæ¥ï¼šdemo.launch(server_name="0.0.0.0", server_port=7860, show_error=True)
    demo.launch(
        server_name="127.0.0.1",   # ç”¨å›ç¯åœ°å€ï¼Œç»•è¿‡æŸäº›ä»£ç†/å®‰å…¨è½¯ä»¶å¯¹ localhost çš„åŠ«æŒ
        server_port=5000,
        show_error=True,
        inbrowser=True              # å¯é€‰ï¼šè‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨
    )